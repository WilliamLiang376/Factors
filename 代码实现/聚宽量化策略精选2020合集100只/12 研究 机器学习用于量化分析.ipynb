{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于机器学习算法的因子选股策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试机器学习算法\n",
    "不同的数据清洗方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 风险及免责提示：该策略由聚宽用户在聚宽社区分享，仅供学习交流使用。\n",
    "# 原文一般包含策略说明，如有疑问请到原文和作者交流讨论。\n",
    "# 原文网址：https://www.joinquant.com/view/community/detail/26406\n",
    "# 标题：机器学习用于量化分析\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import jqdata\n",
    "from jqdata import *\n",
    "import time\n",
    "import datetime\n",
    "from jqfactor import *\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score  # 导入交叉检验算法\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif  # 导入特征选择方法库\n",
    "from sklearn.pipeline import Pipeline  # 导入Pipeline库\n",
    "from sklearn.metrics import accuracy_score  # 准确率指标\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from jqlib.technical_analysis import *\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#按月区间取值\n",
    "peroid = 'M'\n",
    "#样本区间（训练集+测试集的区间为2014-1-31到2018-12-31）\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2020-02-28'\n",
    "#训练集长度\n",
    "train_length = 48\n",
    "#使用行业\n",
    "industry_name = 'sw_l1'  #申万一级行业\n",
    "#聚宽一级行业\n",
    "industry_code = ['HY001', 'HY002', 'HY003', 'HY004', 'HY005', 'HY006', 'HY007', 'HY008', 'HY009', 'HY010', 'HY011']\n",
    "#去除上市不满足N天的股票\n",
    "DELETE_STOP_NDAYS = 90 \n",
    "#股票池，获取中证全指\n",
    "SELECT_STOCK_INDEX = '000985.XSHG'\n",
    "#训练样本比例\n",
    "TRAIN_PERCENT = 0.7\n",
    "#参与分类计算的数据比例\n",
    "POSITIVE_PERCENT = 0.3 #从训练样本中选取收益值最高的前30%标签设为1\n",
    "NAGATIVE_PERCENT = 0.3 #从训练样本中选取收益值最底的后30%标签设为-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#聚宽因子\n",
    "jqfactors_list = ['current_ratio',\n",
    "                  'net_profit_to_total_operate_revenue_ttm',\n",
    "                  'gross_income_ratio',\n",
    "                  'roe_ttm',\n",
    "                  'roa_ttm',\n",
    "                  'total_asset_turnover_rate',\\\n",
    "                  'net_operating_cash_flow_coverage',\n",
    "                  'net_operate_cash_flow_ttm',\n",
    "                  'net_profit_ttm',\\\n",
    "                  'cash_to_current_liability',\n",
    "                  'operating_revenue_growth_rate',\n",
    "                  'non_recurring_gain_loss',\\\n",
    "                  'operating_revenue_ttm',\n",
    "                  'net_profit_growth_rate']\n",
    "\n",
    "basis_factors = ['net_working_capital','total_operating_revenue_ttm','operating_profit_ttm',\n",
    "                'net_operate_cash_flow_ttm','operating_revenue_ttm','interest_carry_current_liability',\n",
    "                'sale_expense_ttm','retained_earnings','total_operating_cost_ttm','non_operating_net_profit_ttm',\n",
    "                'net_invest_cash_flow_ttm','financial_expense_ttm','administration_expense_ttm',\n",
    "                'net_interest_expense','value_change_profit_ttm','total_profit_ttm','net_finance_cash_flow_ttm',\n",
    "                'interest_free_current_liability','EBIT','net_profit_ttm','OperateNetIncome',\n",
    "                'EBITDA','asset_impairment_loss_ttm','np_parent_company_owners_ttm',\n",
    "                'operating_cost_ttm','net_debt','non_recurring_gain_loss','goods_sale_and_service_render_cash_ttm',\n",
    "                'market_cap','cash_flow_to_price_ratio','sales_to_price_ratio','circulating_market_cap',\n",
    "                'operating_assets','financial_assets','operating_liability','financial_liability']\n",
    "\n",
    "#自定义因子\n",
    "q = query(valuation.code, \n",
    "      valuation.market_cap,#市值\n",
    "      valuation.circulating_market_cap,\n",
    "      valuation.pe_ratio, #市盈率（TTM）\n",
    "      valuation.pb_ratio, #市净率（TTM）\n",
    "      valuation.pcf_ratio, #CFP\n",
    "      valuation.ps_ratio, #PS\n",
    "      balance.total_assets,\n",
    "      balance.total_liability,\n",
    "      balance.development_expenditure, #RD\n",
    "      balance.dividend_payable,\n",
    "      balance.fixed_assets,  \n",
    "      balance.total_non_current_liability,\n",
    "      income.operating_profit,\n",
    "      income.total_profit, #OPTP\n",
    "      #\n",
    "      indicator.net_profit_to_total_revenue, #净利润/营业总收入\n",
    "      indicator.inc_revenue_year_on_year,  #营业收入增长率（同比）\n",
    "      indicator.inc_net_profit_year_on_year,#净利润增长率（同比）\n",
    "      indicator.roe,\n",
    "      indicator.roa,\n",
    "      indicator.gross_profit_margin #销售毛利率GPM\n",
    "    )\n",
    "\n",
    "#所有聚宽因子\n",
    "all_jqfactors = list(get_all_factors()['factor'].values)\n",
    "len(all_jqfactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、 数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除上市距beginDate不足n天的股票\n",
    "def delete_stop(stocks,beginDate,n):\n",
    "    stockList=[]\n",
    "    beginDate = datetime.datetime.strptime(beginDate, \"%Y-%m-%d\")\n",
    "    for stock in stocks:\n",
    "        start_date=get_security_info(stock).start_date\n",
    "        if start_date<(beginDate-datetime.timedelta(days=n)).date():\n",
    "            stockList.append(stock)\n",
    "    return stockList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#剔除ST股\n",
    "def delete_st(stocks,begin_date):\n",
    "    st_data=get_extras('is_st',stocks, count = 1,end_date=begin_date)\n",
    "    stockList = [stock for stock in stocks if not st_data[stock][0]]\n",
    "    return stockList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "securities_list = delete_stop(get_index_stocks(SELECT_STOCK_INDEX),start_date,DELETE_STOP_NDAYS)\n",
    "securities_list = delete_st(securities_list,start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343\n"
     ]
    }
   ],
   "source": [
    "print(len(securities_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jq_factor(securities,factor_list,date):\n",
    "    '''\n",
    "    获取聚宽因子\n",
    "    securities：list,股票列表\n",
    "    factor_list:list,因子列表\n",
    "    date: 日期， 字符串或 datetime 对象\n",
    "    output:\n",
    "    dataframe, index为股票代码，columns为因子\n",
    "    '''\n",
    "    factor_data = get_factor_values(securities=securities, \\\n",
    "                                    factors=factor_list, \\\n",
    "                                    count=1, \\\n",
    "                                    end_date=date)\n",
    "    df_jq_factor=pd.DataFrame(index=securities)\n",
    "    \n",
    "    for i in factor_data.keys():\n",
    "        df_jq_factor[i]=factor_data[i].iloc[0,:]\n",
    "    \n",
    "    return df_jq_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_df(securities,df,date):\n",
    "    \n",
    "    #净资产\n",
    "    df['net_assets']=df['total_assets']-df['total_liability']\n",
    "\n",
    "    df_new = pd.DataFrame(index=securities)\n",
    "        \n",
    "    #估值因子\n",
    "    df_new['EP'] = df['pe_ratio'].apply(lambda x: 1/x)\n",
    "    df_new['BP'] = df['pb_ratio'].apply(lambda x: 1/x)\n",
    "    df_new['SP'] = df['ps_ratio'].apply(lambda x: 1/x)\n",
    "    #df_new['DP'] = df['dividend_payable']/(df['market_cap']*100000000)\n",
    "    df_new['RD'] = df['development_expenditure']/(df['market_cap']*100000000)\n",
    "    df_new['CFP'] = df['pcf_ratio'].apply(lambda x: 1/x)\n",
    "    \n",
    "    #杠杆因子\n",
    "    #对数流通市值\n",
    "    df_new['CMV'] = np.log(df['circulating_market_cap'])\n",
    "    #总资产/净资产\n",
    "    df_new['financial_leverage']=df['total_assets']/df['net_assets']\n",
    "    #非流动负债/净资产\n",
    "    df_new['debtequityratio']=df['total_non_current_liability']/df['net_assets']\n",
    "    #现金比率=(货币资金+有价证券)÷流动负债\n",
    "    df_new['cashratio']=df['cash_to_current_liability']\n",
    "    #流动比率=流动资产/流动负债*100%\n",
    "    df_new['currentratio']=df['current_ratio']\n",
    "    \n",
    "    #财务质量因子\n",
    "    # 净利润与营业总收入之比\n",
    "    df_new['NI'] = df['net_profit_to_total_operate_revenue_ttm']\n",
    "    df_new['GPM'] = df['gross_income_ratio']\n",
    "    df_new['ROE'] = df['roe_ttm']\n",
    "    df_new['ROA'] = df['roa_ttm']\n",
    "    df_new['asset_turnover'] = df['total_asset_turnover_rate']\n",
    "    df_new['net_operating_cash_flow'] = df['net_operating_cash_flow_coverage']\n",
    "    \n",
    "    #成长因子\n",
    "    df_new['Sales_G_q'] = df['operating_revenue_growth_rate']\n",
    "    df_new['Profit_G_q'] = df['net_profit_growth_rate']\n",
    "    \n",
    "    #技术指标\n",
    "    df_new['RSI']=pd.Series(RSI(securities, date, N1=20))    \n",
    "    dif,dea,macd=MACD(securities, date, SHORT = 10, LONG = 30, MID = 15)\n",
    "    df_new['DIF']=pd.Series(dif)\n",
    "    df_new['DEA']=pd.Series(dea)\n",
    "    df_new['MACD']=pd.Series(macd)    \n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取指定周期的日期列表 'W、M、Q'\n",
    "def get_period_date(peroid,start_date, end_date):\n",
    "    #设定转换周期period_type  转换为周是'W',月'M',季度线'Q',五分钟'5min',12天'12D'\n",
    "    stock_data = get_price('000001.XSHE',start_date,end_date,'daily',fields=['close'])\n",
    "    #记录每个周期中最后一个交易日\n",
    "    stock_data['date']=stock_data.index\n",
    "    #进行转换，周线的每个变量都等于那一周中最后一个交易日的变量值\n",
    "    period_stock_data=stock_data.resample(peroid).last()\n",
    "    date = period_stock_data.index\n",
    "    pydate_array = date.to_pydatetime()\n",
    "    date_only_array = np.vectorize(lambda s: s.strftime('%Y-%m-%d'))(pydate_array )\n",
    "    date_only_series = pd.Series(date_only_array)\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    start_date = start_date-datetime.timedelta(days=1)\n",
    "    start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    date_list = date_only_series.values.tolist()\n",
    "    date_list.insert(0,start_date)\n",
    "    return date_list\n",
    "dateList = get_period_date(peroid,start_date,end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datelist_stocks_factors(securities,jqfactot_list,date_list):\n",
    "    # (jqdata)因子\n",
    "    df_jq_factor = {}\n",
    "    # （财务数据）因子\n",
    "    df_q_factor = {}\n",
    "    # 预处理前的原始因子训练集\n",
    "    df_factor_pre_train = {}\n",
    "\n",
    "    for date in dateList:\n",
    "        df_jq_factor = get_jq_factor(securities,jqfactot_list,date)\n",
    "        df_q_factor = get_fundamentals(q.filter(valuation.code.in_(securities)), date = date)\n",
    "        df_q_factor.index = df_q_factor['code']\n",
    "        # 合并得大表\n",
    "        df_factor_pre_train[date] = pd.concat([df_q_factor,df_jq_factor],axis=1)\n",
    "        # 初始化\n",
    "        df_factor_pre_train[date] = initialize_df(securities,df_factor_pre_train[date],date)\n",
    "        \n",
    "    return df_factor_pre_train\n",
    "\n",
    "#df_factor_pre_train = get_datelist_stocks_factors(securities_list,jqfactors_list,dateList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datelist_stocks_factors_jqfactors(securities,jqfactot_list,date_list):\n",
    "    # (jqdata)因子\n",
    "    df_jq_factor = {}\n",
    "    # （财务数据）因子\n",
    "    df_q_factor = {}\n",
    "    # 预处理前的原始因子训练集\n",
    "    df_factor_pre_train = {}\n",
    "\n",
    "    for date in date_list:      \n",
    "        length_factors = len(jqfactot_list)\n",
    "        if length_factors > 50:\n",
    "            df_factor_pre_train_l = []\n",
    "            r = int(length_factors/30)\n",
    "            for i in range(r+1):\n",
    "                if 30*(i+1) < length_factors:\n",
    "                    df = get_jq_factor(securities,jqfactot_list[30*i:30*(i+1)],date)\n",
    "                else:\n",
    "                    df = get_jq_factor(securities,jqfactot_list[30*i:],date)\n",
    "                df_factor_pre_train_l.append(df)\n",
    "            df_jq_factor = pd.concat(df_factor_pre_train_l,axis=1)\n",
    "        else:\n",
    "            df_jq_factor = get_jq_factor(securities,jqfactot_list,date)\n",
    "\n",
    "        df_factor_pre_train[date] = df_jq_factor\n",
    "\n",
    "    return df_factor_pre_train\n",
    "\n",
    "#df_factor_pre_train = get_datelist_stocks_factors_jqfactors(securities_list,all_jqfactors,dateList[125:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_jqfactors.pkl','rb') as pf:\n",
    "    all_data = pickle.load(pf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#提取基础因子\n",
    "keys = list(all_data.keys())\n",
    "basic_dic = {}\n",
    "for key in keys[:2]:\n",
    "    data = all_data[key]\n",
    "    basic_data = data[basis_factors]\n",
    "    basic_dic[key] = basic_data\n",
    "len(basic_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_stock_industry(industry_name,date,output_csv = False):\n",
    "    '''\n",
    "    获取股票对应的行业\n",
    "    input：\n",
    "    industry_name: str, \n",
    "    \"sw_l1\": 申万一级行业\n",
    "    \"sw_l2\": 申万二级行业\n",
    "    \"sw_l3\": 申万三级行业\n",
    "    \"jq_l1\": 聚宽一级行业\n",
    "    \"jq_l2\": 聚宽二级行业\n",
    "    \"zjw\": 证监会行业\n",
    "    date:时间\n",
    "    output: DataFrame,index 为股票代码，columns 为所属行业代码\n",
    "    '''\n",
    "    industries = list(get_industries(industry_name).index)\n",
    "    all_securities = get_all_securities(date=date)   #获取当天所有股票代码\n",
    "    all_securities['industry_code'] = 1\n",
    "    for ind in industries:\n",
    "        industry_stocks = get_industry_stocks(ind,date)\n",
    "        #有的行业股票不在all_stocks列表之中\n",
    "        industry_stocks = set(all_securities) & set(industry_stocks)\n",
    "        all_securities['industry_code'][industry_stocks] = ind\n",
    "    stock_industry = all_securities['industry_code'].to_frame()\n",
    "    if output_csv == True:\n",
    "        stock_industry.to_csv('stock_industry.csv') #输出csv文件，股票对应行业\n",
    "    return stock_industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def fillna_with_industry(data,date,industry_name='sw_l1'):\n",
    "    '''\n",
    "    使用行业均值填充nan值\n",
    "    input:\n",
    "    data：DataFrame,输入数据，index为股票代码\n",
    "    date:string,时间必须和data数值对应时间一致\n",
    "    output：\n",
    "    DataFrame,缺失值用行业中值填充，无行业数据的用列均值填充\n",
    "    '''\n",
    "    stocks = list(data.index)\n",
    "    stocks_industry = get_stock_industry(industry_name,date)\n",
    "    stocks_industry_merge = data.merge(stocks_industry, left_index=True,right_index=True,how='left')\n",
    "    stocks_dropna = stocks_industry_merge.dropna()\n",
    "    columns = list(data.columns)\n",
    "    select_data = []\n",
    "    group_data = stocks_industry_merge.groupby('industry_code')\n",
    "    group_data_mean = group_data.mean()\n",
    "    group_data = stocks_industry_merge.merge(group_data_mean,left_on='industry_code',right_index=True,how='left')\n",
    "    for column in columns:\n",
    "\n",
    "        if type(data[column][0]) != str:\n",
    "\n",
    "            group_data[column+'_x'][pd.isnull(group_data[column+'_x'])] = group_data[column+'_y'][pd.isnull(group_data[column+'_x'])]\n",
    "            \n",
    "            group_data[column] = group_data[column+'_x']\n",
    "            #print(group_data.head())\n",
    "            select_data.append(group_data[column])\n",
    "            \n",
    "    result = pd.concat(select_data,axis=1)\n",
    "    #行业均值为Nan,用总体均值填充\n",
    "    mean = result.mean()\n",
    "    for i in result.columns:\n",
    "        result[i].fillna(mean[i],inplace=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "def data_preprocessing(factor_data,date,industry_name='sw_l1'):\n",
    "    '''\n",
    "    数据预处理\n",
    "    input:\n",
    "    factor_data:df ，index为股票列表，columns为因子\n",
    "    \n",
    "    '''\n",
    "    #去极值\n",
    "    try:\n",
    "        factor_data=winsorize(factor_data, inf2nan=False,scale=1,axis=0)\n",
    "\n",
    "        #缺失值处理\n",
    "        factor_data=fillna_with_industry(factor_data,date,industry_name=industry_name)\n",
    "        #中性化处理\n",
    "        factor_data=neutralize(factor_data,how=['sw_l1','ln_market_cap'], date=date, axis=0)\n",
    "        #标准化处理\n",
    "        factor_data=standardlize(factor_data,axis=0)\n",
    "    except:\n",
    "        print(factor_data)\n",
    "        print(date)\n",
    "    return factor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_fac = basic_dic\\n \\ndate_list =  list(all_fac.keys())\\nneu_dic = {}\\nst = time.time()\\ni=0\\nfor date in date_list:\\n    data = all_fac[date]\\n    data_neu = data_preprocessing(data,date)\\n    neu_dic[date] = data_neu\\n    et = time.time()\\n    print((et-st)/60)\\n    i = i + 1\\n    print(i)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "all_fac = basic_dic\n",
    " \n",
    "date_list =  list(all_fac.keys())\n",
    "neu_dic = {}\n",
    "st = time.time()\n",
    "i=0\n",
    "for date in date_list:\n",
    "    data = all_fac[date]\n",
    "    data_neu = data_preprocessing(data,date)\n",
    "    neu_dic[date] = data_neu\n",
    "    et = time.time()\n",
    "    print((et-st)/60)\n",
    "    i = i + 1\n",
    "    print(i)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('all_jqfactors_neu_ln_market_cap_swl1.pkl','rb') as pf:\\n    all_data = pickle.load(pf)\\n\\nwith open('basic_data_neu_ln_market_cap_swl1.pkl','rb') as pf:\\n    basic_data = pickle.load(pf)\\nkeys = list(all_data.keys())\\ncombine_dic = {}\\nfor key in keys:\\n    all_df = all_data[key]\\n    basic_df = basic_data[key]\\n    all_df[basis_factors] = basic_df\\n    combine_dic[key] = all_df\\nlen(combine_dic)\\nwith open('combine_neu_ln_market_cap_swl1.pkl','wb') as pf:\\n    pickle.dump(combine_dic,pf)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open('all_jqfactors_neu_ln_market_cap_swl1.pkl','rb') as pf:\n",
    "    all_data = pickle.load(pf)\n",
    "\n",
    "with open('basic_data_neu_ln_market_cap_swl1.pkl','rb') as pf:\n",
    "    basic_data = pickle.load(pf)\n",
    "keys = list(all_data.keys())\n",
    "combine_dic = {}\n",
    "for key in keys:\n",
    "    all_df = all_data[key]\n",
    "    basic_df = basic_data[key]\n",
    "    all_df[basis_factors] = basic_df\n",
    "    combine_dic[key] = all_df\n",
    "len(combine_dic)\n",
    "with open('combine_neu_ln_market_cap_swl1.pkl','wb') as pf:\n",
    "    pickle.dump(combine_dic,pf)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_factor_train = {}\\nfor date in dateList:\\n    try:\\n        df_factor_train[date] = data_preprocessing(df_factor_pre_train[date],date)\\n    except:\\n        print(date)\\n\\nwith open('df_factor_after_process_10_20_all_jqfactors.pkl','wb') as pf:\\n        pickle.dump(df_factor_train,pf)\\n\\n\\n    \\nwith open('df_factor_after_process_10_20_all_jqfactors.pkl','rb') as pf:\\n    df_factor_train = pickle.load(pf)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预处理后的原始因子训练集\n",
    "'''\n",
    "df_factor_train = {}\n",
    "for date in dateList:\n",
    "    try:\n",
    "        df_factor_train[date] = data_preprocessing(df_factor_pre_train[date],date)\n",
    "    except:\n",
    "        print(date)\n",
    "\n",
    "with open('df_factor_after_process_10_20_all_jqfactors.pkl','wb') as pf:\n",
    "        pickle.dump(df_factor_train,pf)\n",
    "\n",
    "\n",
    "    \n",
    "with open('df_factor_after_process_10_20_all_jqfactors.pkl','rb') as pf:\n",
    "    df_factor_train = pickle.load(pf)\n",
    "'''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combine_neu_ln_market_cap_swl1.pkl','rb') as pf:\n",
    "    df_factor_train = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集和交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 训练集数据\n",
    "def get_train_data(dic_data,date_list):\n",
    "    '''\n",
    "    input:\n",
    "    dic_data:dic,keys为时间，values为df,index为股票列表，columns为因子\n",
    "    date_list:list，时间列表，必须和dic_data的keys对应\n",
    "    '''\n",
    "    length = len(dic_data)\n",
    "    train_length = int(length * TRAIN_PERCENT)\n",
    "    train_data=pd.DataFrame()\n",
    "    for date in dateList[:train_length]:\n",
    "        traindf=df_factor_train[date]\n",
    "        stockList=list(traindf.index)\n",
    "        #取收益率数据\n",
    "        data_close=get_price(stockList,date,dateList[dateList.index(date)+1],'1d','close')['close']\n",
    "        traindf['pchg']=data_close.iloc[-1]/data_close.iloc[0]-1\n",
    "        #剔除空值\n",
    "        traindf=traindf.dropna() \n",
    "        #traindf=traindf.sort(columns='pchg')\n",
    "        traindf=traindf.sort_values(by=['pchg'],ascending=False)\n",
    "        \n",
    "        #选取前后各30%的股票，剔除中间的噪声\n",
    "        #取0-30%+70%-100%的数据\n",
    "        traindf=traindf.iloc[:int(len(traindf['pchg'])*POSITIVE_PERCENT),:].append(traindf.iloc[int(len(traindf['pchg'])*(1-NAGATIVE_PERCENT)):,:])\n",
    "        #前30%为1，后30%为-1\n",
    "        traindf['label']=list(traindf['pchg'].apply(lambda x:1 if x>np.mean(list(traindf['pchg'])) else -1))    \n",
    "        if train_data.empty:\n",
    "            train_data=traindf\n",
    "        else:\n",
    "            train_data=train_data.append(traindf)\n",
    "    return train_data\n",
    "train_data = get_train_data(df_factor_train,dateList)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66010"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_test_data(dic_data,date_list):\n",
    "    '''\n",
    "    input:\n",
    "    dic_data:dic,keys为时间，values为df,index为股票列表，columns为因子\n",
    "    date_list:list，时间列表，必须和dic_data的keys对应\n",
    "    output:\n",
    "    dic,keys为时间，values为df，index为股票列表，column为因子，其中最后一列是标签值，\n",
    "    '''\n",
    "    length = len(dic_data)\n",
    "    train_length = int(length * TRAIN_PERCENT)\n",
    "    test_data={}\n",
    "    for date in dateList[train_length:-1]:\n",
    "        testdf=df_factor_train[date]\n",
    "        stockList=list(testdf.index)\n",
    "        # 取收益率数据\n",
    "        data_close=get_price(stockList,date,dateList[dateList.index(date)+1],'1d','close')['close']\n",
    "        testdf['pchg']=data_close.iloc[-1]/data_close.iloc[0]-1\n",
    "        #剔除空值\n",
    "        testdf=testdf.dropna()  \n",
    "    \n",
    "        testdf=testdf.sort_values(by=['pchg'],ascending=False)\n",
    "        #选取前后各30%的股票，剔除中间的噪声\n",
    "        #取0-30%+70%-100%的数据\n",
    "        #testdf=testdf.iloc[:int(len(traindf['pchg'])/10*3),:].append(testdf.iloc[int(len(testdf['pchg'])/10*7):,:])\n",
    "        testdf['label']=list(testdf['pchg'].apply(lambda x:1 if x>0 else -1)) \n",
    "        test_data[date]=testdf\n",
    "    return test_data\n",
    "\n",
    "test_data = get_test_data(df_factor_train,dateList)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集\n",
    "y_train = train_data['label']  # 分割y\n",
    "X_train = train_data.copy()\n",
    "del X_train['pchg']\n",
    "del X_train['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_l = []\n",
    "for key in test_data.keys():\n",
    "    td = test_data[key]\n",
    "    test_l.append(td)\n",
    "test_df = pd.concat(test_l)\n",
    "y_test = test_df['label']\n",
    "x_test = test_df.copy()\n",
    "del x_test['pchg']\n",
    "del x_test['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FeatureSelection():\n",
    "    '''\n",
    "    特征选择：\n",
    "    identify_collinear：基于相关系数，删除小于correlation_threshold的特征\n",
    "    identify_importance_lgbm：基于LightGBM算法，得到feature_importance,选择和大于p_importance的特征\n",
    "    filter_select:单变量选择，指定k,selectKBest基于method提供的算法选择前k个特征，\n",
    "    selectPercentile选择前p百分比的特征\n",
    "    wrapper_select:RFE，基于estimator递归特征消除，保留n_feature_to_select个特征\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.supports = None #bool型，特征是否被选中\n",
    "        self.columns = None  #选择的特征\n",
    "        self.record_collinear = None #自相关矩阵大于门限值\n",
    "        \n",
    "    def identify_collinear(self, data, correlation_threshold):\n",
    "        \"\"\"\n",
    "        Finds collinear features based on the correlation coefficient between features. \n",
    "        For each pair of features with a correlation coefficient greather than `correlation_threshold`,\n",
    "        only one of the pair is identified for removal. \n",
    "\n",
    "        Using code adapted from: https://gist.github.com/Swarchal/e29a3a1113403710b6850590641f046c\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "\n",
    "        data : dataframe\n",
    "            Data observations in the rows and features in the columns\n",
    "\n",
    "        correlation_threshold : float between 0 and 1\n",
    "            Value of the Pearson correlation cofficient for identifying correlation features\n",
    "\n",
    "        \"\"\"\n",
    "        columns = data.columns\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "\n",
    "        # Calculate the correlations between every column\n",
    "        corr_matrix = data.corr()\n",
    "        \n",
    "        self.corr_matrix = corr_matrix\n",
    "    \n",
    "        # Extract the upper triangle of the correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "        # Select the features with correlations above the threshold\n",
    "        # Need to use the absolute value\n",
    "        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "        obtain_columns = [column for column in columns if column not in to_drop]\n",
    "        self.columns = obtain_columns\n",
    "        # Dataframe to hold correlated pairs\n",
    "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        # Iterate through the columns to drop\n",
    "        for column in to_drop:\n",
    "\n",
    "            # Find the correlated features\n",
    "            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "            # Find the correlated values\n",
    "            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "            # Record the information (need a temp df for now)\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                             'corr_feature': corr_features,\n",
    "                                             'corr_value': corr_values})\n",
    "\n",
    "            # Add to dataframe\n",
    "            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "\n",
    "        self.record_collinear = record_collinear\n",
    "        return data[obtain_columns]\n",
    "     \n",
    "        \n",
    "    def identify_importance_lgbm(self, features, labels,p_importance=0.8, eval_metric='auc', task='classification', \n",
    "                                 n_iterations=10, early_stopping = True):\n",
    "        \"\"\"\n",
    "        \n",
    "        Identify the features with zero importance according to a gradient boosting machine.\n",
    "        The gbm can be trained with early stopping using a validation set to prevent overfitting. \n",
    "        The feature importances are averaged over n_iterations to reduce variance. \n",
    "        \n",
    "        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "        Parameters \n",
    "        --------\n",
    "        features : dataframe\n",
    "            Data for training the model with observations in the rows\n",
    "            and features in the columns\n",
    "\n",
    "        labels : array, shape = (1, )\n",
    "            Array of labels for training the model. These can be either binary \n",
    "            (if task is 'classification') or continuous (if task is 'regression')\n",
    "            \n",
    "        p_importance:float, range[0,1],default = 0.8\n",
    "            sum of the importance of features above the value\n",
    "\n",
    "        eval_metric : string\n",
    "            Evaluation metric to use for the gradient boosting machine\n",
    "\n",
    "        task : string, default = 'classification'\n",
    "            The machine learning task, either 'classification' or 'regression'\n",
    "\n",
    "        n_iterations : int, default = 10\n",
    "            Number of iterations to train the gradient boosting machine\n",
    "            \n",
    "        early_stopping : boolean, default = True\n",
    "            Whether or not to use early stopping with a validation set when training\n",
    "        \n",
    "        \n",
    "        Notes\n",
    "        --------\n",
    "        \n",
    "        - Features are one-hot encoded to handle the categorical variables before training.\n",
    "        - The gbm is not optimized for any particular task and might need some hyperparameter tuning\n",
    "        - Feature importances, including zero importance features, can change across runs\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # One hot encoding\n",
    "        data = features\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        # Extract feature names\n",
    "        feature_names = list(features.columns)\n",
    "\n",
    "        # Convert to np array\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels).reshape((-1, ))\n",
    "\n",
    "        # Empty array for feature importances\n",
    "        feature_importance_values = np.zeros(len(feature_names))\n",
    "        \n",
    "        print('Training Gradient Boosting Model\\n')\n",
    "        \n",
    "        # Iterate through each fold\n",
    "        for _ in range(n_iterations):\n",
    "\n",
    "            if task == 'classification':\n",
    "                model = lgb.LGBMClassifier(n_estimators=100, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            elif task == 'regression':\n",
    "                model = lgb.LGBMRegressor(n_estimators=100, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Task must be either \"classification\" or \"regression\"')\n",
    "                \n",
    "            # If training using early stopping need a validation set\n",
    "            if early_stopping:\n",
    "                \n",
    "                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15)\n",
    "\n",
    "                # Train the model with early stopping\n",
    "                model.fit(train_features, train_labels, eval_metric = eval_metric,\n",
    "                          eval_set = [(valid_features, valid_labels)],\n",
    "                           verbose = -1)\n",
    "                \n",
    "                # Clean up memory\n",
    "                gc.enable()\n",
    "                del train_features, train_labels, valid_features, valid_labels\n",
    "                gc.collect()\n",
    "                \n",
    "            else:\n",
    "                model.fit(features, labels)\n",
    "\n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_ / n_iterations\n",
    "\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "        # Sort features according to importance\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "        # Normalize the feature importances to add up to one\n",
    "        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "        select_df = feature_importances[feature_importances['cumulative_importance']<=p_importance]\n",
    "        select_columns = select_df['feature']\n",
    "        self.columns = list(select_columns.values)\n",
    "        res = data[self.columns]\n",
    "        return res\n",
    "        \n",
    "    def filter_select(self, data_x, data_y, k=None, p=50,method=f_classif):\n",
    "        columns = data_x.columns\n",
    "        if k != None:\n",
    "            model = SelectKBest(method,k)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        else:\n",
    "            model = SelectPercentile(method,p)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        self.support_ = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def wrapper_select(self,data_x,data_y,n,estimator):\n",
    "        columns = data_x.columns\n",
    "        model = RFE(estimator=estimator,n_features_to_select=n)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support() #标识被选择的特征在原数据中的位置\n",
    "        self.supports = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def embedded_select(self,data_x,data_y,estimator,threshold=None):\n",
    "        '''\n",
    "        threshold : string, float, optional default None\n",
    "        The threshold value to use for feature selection. Features whose importance is greater or\n",
    "        equal are kept while the others are discarded. If “median” (resp. “mean”), then the \n",
    "        threshold value is the median (resp. the mean) of the feature importances. \n",
    "        A scaling factor (e.g., “1.25*mean”) may also be used. If None and if the estimator\n",
    "        has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso),\n",
    "        the threshold used is 1e-5. Otherwise, “mean” is used by default.\n",
    "        '''\n",
    "        columns = data_x.columns\n",
    "        model = SelectFromModel(estimator=estimator,prefit=False,threshold=threshold)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support()\n",
    "        self.supports = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 选择最佳特征比例\n",
    "# #############################################################################\n",
    "# Plot the cross-validation score as a function of percentile of features\n",
    "def feature_selection_params_xgboost():\n",
    "    score_means = list()\n",
    "    score_stds = list()\n",
    "    importance_list = [0.5,0.6,0.7,0.8,0.9,1]\n",
    "    model = XGBClassifier()\n",
    "    featureSelect = FeatureSelection()\n",
    "    for importance in importance_list:\n",
    "        lgbm_res = featureSelect.identify_importance_lgbm(X_train,y_train,p_importance=importance)\n",
    "\n",
    "        this_scores = cross_val_score(model, lgbm_res, y_train, cv=5, n_jobs=-1)\n",
    "        score_means.append(this_scores.mean())\n",
    "        score_stds.append(this_scores.std())\n",
    "    plt.errorbar(importance_list, score_means, np.array(score_stds))\n",
    "    plt.title('Performance of the p_importance of features selected')\n",
    "    plt.xlabel('importance')\n",
    "    plt.ylabel('Prediction rate')\n",
    "    plt.axis('tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 选择最佳特征比例\n",
    "# #############################################################################\n",
    "# Plot the cross-validation score as a function of percentile of features\n",
    "def feature_selection_params_randomforest():\n",
    "    score_means = list()\n",
    "    score_stds = list()\n",
    "    importance_list = [0.5,0.6,0.7,0.8,0.9,1]\n",
    "    model = RandomForestClassifier()\n",
    "    featureSelect = FeatureSelection()\n",
    "    for importance in importance_list:\n",
    "        lgbm_res = featureSelect.identify_importance_lgbm(X_train,y_train,p_importance=importance)\n",
    "\n",
    "        this_scores = cross_val_score(model, lgbm_res, y_train, cv=5, n_jobs=-1)\n",
    "        score_means.append(this_scores.mean())\n",
    "        score_stds.append(this_scores.std())\n",
    "    plt.errorbar(importance_list, score_means, np.array(score_stds))\n",
    "    plt.title('Performance of the p_importance of features selected')\n",
    "    plt.xlabel('importance')\n",
    "    plt.ylabel('Prediction rate')\n",
    "    plt.axis('tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过结果观察，p_importance=0.9时效果最好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting Model\n",
      "\n",
      "178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureSelect = FeatureSelection()\n",
    "\n",
    "lgbm_feature_select = featureSelect.identify_importance_lgbm(X_train,y_train,p_importance=0.9)\n",
    "\n",
    "print(len(featureSelect.columns))\n",
    "len(X_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5682017876079382\n",
      "{'max_depth': 7, 'min_samples_split': 120, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "model_params = {'n_estimators':range(4,12,3),'max_depth':range(3,8,2),'min_samples_split':range(20,300,50)}\n",
    "rf_grid_serach_model = GridSearchCV(estimator=rf_model,param_grid=model_params)\n",
    "rf_grid_serach_model.fit(lgbm_feature_select,y_train)\n",
    "print(rf_grid_serach_model.best_score_)\n",
    "print(rf_grid_serach_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5657779124375095\n",
      "{'max_depth': 4, 'min_samples_split': 230, 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "model_params = {'n_estimators':range(3,6),'max_depth':range(2,5),'min_samples_split':range(230,300,20)}\n",
    "rf_grid_serach_model = GridSearchCV(estimator=rf_model,param_grid=model_params)\n",
    "rf_grid_serach_model.fit(lgbm_feature_select,y_train)\n",
    "print(rf_grid_serach_model.best_score_)\n",
    "print(rf_grid_serach_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy    0.493443\n",
      "roc_auc     0.506198\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#测试集预测\n",
    "def test_prediction_score(test_data,select_factors,model):\n",
    "    '''\n",
    "    input:\n",
    "    test_data:dic,keys为时间，values为dataframe,index为样本，columns为因子\n",
    "    select_factors:特征选择的因子\n",
    "    model：训练好的模型\n",
    "    output:\n",
    "    \n",
    "    '''\n",
    "    test_data_keys = list(test_data.keys())\n",
    "    accuracy_score_list = []\n",
    "    roc_auc_score_list = []\n",
    "    date_list = []\n",
    "    for date in test_data_keys:\n",
    "        test_d = test_data[date]\n",
    "        if len(test_d) == 0:\n",
    "            break\n",
    "        y_test = test_d['label']\n",
    "        x_test = test_d.copy()\n",
    "        del x_test['pchg']\n",
    "        del x_test['label']\n",
    "        x_test_featurn_select = x_test[select_factors]\n",
    "        prediction = model.predict(x_test_featurn_select)\n",
    "        accuracy_score_res = accuracy_score(y_test,prediction)\n",
    "        roc_auc_score_res = roc_auc_score(y_test,prediction)\n",
    "        accuracy_score_list.append(accuracy_score_res)\n",
    "        roc_auc_score_list.append(roc_auc_score_res)\n",
    "        date_list.append(date)\n",
    "    score_df = pd.DataFrame(accuracy_score_list,index=date_list,columns=['accuracy'])\n",
    "    score_df['roc_auc'] = roc_auc_score_list\n",
    "    return score_df\n",
    "select_factors = featureSelect.columns\n",
    "test_model = rf_grid_serach_model\n",
    "score_df = test_prediction_score(test_data,select_factors,test_model)\n",
    "print(score_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "xgboost算法\n",
    "\n",
    "第一次执行：n_estimators：12，max_depth：3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5631116497500379\n",
      "{'max_depth': 5, 'n_estimators': 12}\n",
      "用时： 15.614442876974742\n"
     ]
    }
   ],
   "source": [
    "def xgboost_model(x_train,y_train):\n",
    "    st = time.time()\n",
    "    xgboost_model = XGBClassifier()\n",
    "    model_params = {'n_estimators':range(6,14,2),'max_depth':range(3,8,2)}\n",
    "    model = GridSearchCV(estimator=xgboost_model,param_grid=model_params,cv=5)\n",
    "    model.fit(x_train,y_train)\n",
    "    et = time.time()\n",
    "    delta = (et - st) / 60\n",
    "    print(model.best_score_)\n",
    "    print(model.best_params_)\n",
    "    print('用时：',delta)\n",
    "    return model\n",
    "xgboost_model = xgboost_model(lgbm_feature_select,y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy    0.497290\n",
      "roc_auc     0.513936\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "xgboost_score_df = test_prediction_score(test_data,select_factors,xgboost_model)\n",
    "print(xgboost_score_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48348,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n",
      "train loss is 0: 1.25543\n",
      "test loss is1.457069\n",
      "train loss is 1: 1.2518542\n",
      "test loss is1.4520023\n",
      "train loss is 2: 1.2518482\n",
      "test loss is1.4519938\n",
      "train loss is 3: 1.2518482\n",
      "test loss is1.4519938\n",
      "train loss is 4: 1.2518482\n",
      "test loss is1.4519938\n",
      "train loss is 5: 1.2518482\n",
      "test loss is1.4519938\n",
      "train loss is 6: 1.2518482\n",
      "test loss is1.4519938\n",
      "train loss is 7: 1.2518482\n",
      "test loss is1.4519938\n",
      "train loss is 8: 1.2518482\n",
      "test loss is1.4519938\n",
      "train loss is 9: 1.2518482\n",
      "test loss is1.4519938\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def nn_model(x_train,y_train,x_test,y_test):\n",
    "    keep_prob = 0.5\n",
    "    LR = 0.1\n",
    "    l1_num = 6\n",
    "    l2_num = 5\n",
    "    batch_size = 10\n",
    "    times = 1000\n",
    "\n",
    "    v1 = np.array(y_train.values)\n",
    "    train_y_a = v1.reshape((-1,1))\n",
    "    v2 = np.array(y_test.values)\n",
    "    test_y_a = v2.reshape((-1,1))\n",
    "\n",
    "    n_features = len(x_train.columns)\n",
    "    print(n_features)\n",
    "    x = tf.placeholder(tf.float32,[None,n_features])\n",
    "    y = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "    def weights(shape):\n",
    "        weights = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "        return weights\n",
    "    def biases(shape):\n",
    "        biases = tf.zeros(shape) + 0.1\n",
    "        return tf.Variable(biases)\n",
    "\n",
    "    weights1 = weights([n_features,l1_num])\n",
    "    biases1 = biases([1,l1_num])\n",
    "    w_plus_b1 = tf.matmul(x,weights1) + biases1\n",
    "    l1 = tf.nn.relu(w_plus_b1)\n",
    "    l1_dropout = tf.nn.dropout(l1,keep_prob=keep_prob)\n",
    "\n",
    "    weight2 = weights([l1_num,l2_num])\n",
    "    biases2 = biases([l2_num])\n",
    "    w_plus_b2 = tf.matmul(l1_dropout,weight2) + biases2\n",
    "    l2 = tf.nn.relu(w_plus_b2)\n",
    "    l2_dropout = tf.nn.dropout(l2,keep_prob=keep_prob)\n",
    "\n",
    "    weight3 = weights([l2_num,1])\n",
    "    biases3 = biases([1])\n",
    "    w_plus_b3 = tf.matmul(l2_dropout,weight3) + biases3\n",
    "    l3 = tf.nn.relu(w_plus_b3)\n",
    "    res = tf.nn.sigmoid(l3)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - res))\n",
    "    train_steps = tf.train.AdadeltaOptimizer(LR).minimize(loss)\n",
    "\n",
    "    initialize = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(initialize)\n",
    "        for batch in range(batch_size):\n",
    "            for time in range(times):\n",
    "                sess.run(train_steps,feed_dict={x:x_train,y:train_y_a})\n",
    "            accuracy = sess.run(loss,feed_dict={x:x_train,y:train_y_a})\n",
    "            accuracy_test = sess.run(loss,feed_dict={x:x_test,y:test_y_a})\n",
    "            print('train loss is '+ str(batch) + ': '+ str(accuracy))\n",
    "            print('test loss is' + str(accuracy_test))\n",
    "x_test_feature_select = x_test[featureSelect.columns]   \n",
    "print(len(lgbm_feature_select.columns))\n",
    "nn_model(lgbm_feature_select,y_train,x_test_feature_select,y_test)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "MarkDown菜单",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
