{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一模块 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 风险及免责提示：该策略由聚宽用户在聚宽社区分享，仅供学习交流使用。\n",
    "# 原文一般包含策略说明，如有疑问请到原文和作者交流讨论。\n",
    "# 原文网址：https://www.joinquant.com/view/community/detail/16261\n",
    "# 标题：多因子策略研究代码框架\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "import warnings\n",
    "from jqdata import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此代码在自己机器上运行时使用，在聚宽平台上跑数据不需要\n",
    "#from jqdatasdk import *\n",
    "#auth('用户名','密码')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2014-01-01'\n",
    "\n",
    "\n",
    "all_trade_days = (get_trade_days(start_date=start_date,end_date=end_date)).tolist() #所有交易日\n",
    "trade_days = all_trade_days[::20] #每隔20天取一次数据，基本面数据更新频率较慢，数据获取频率尽量与之对应\n",
    "\n",
    "securities = get_all_securities()\n",
    "start_data_dt = datetime.datetime.strptime(start_date,'%Y-%m-%d').date()\n",
    "securities_after_start_date = securities[(securities['start_date']<start_data_dt)] #选择起始时间之前上市的股票\n",
    "all_stocks = list(securities_after_start_date.index)\n",
    "\n",
    "INDUSTRY_NAME = 'sw_l1'\n",
    "\n",
    "ttm_factors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本面数据及缺失值填充函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "基本面因子映射\n",
    "'''\n",
    "fac_dict = {\n",
    "    'MC':valuation.market_cap, # 总市值\n",
    "    'GP':indicator.gross_profit_margin * income.operating_revenue, # 毛利润\n",
    "    'OP':income.operating_profit,\n",
    "    'OR':income.operating_revenue, # 营业收入\n",
    "    'NP':income.net_profit, # 净利润\n",
    "    'EV':valuation.market_cap + balance.shortterm_loan+balance.non_current_liability_in_one_year+balance.longterm_loan+balance.bonds_payable+balance.longterm_account_payable - cash_flow.cash_and_equivalents_at_end,\n",
    "    \n",
    "    'TOE':balance.total_owner_equities, # 股东权益合计(元)\n",
    "    'TOR':income.total_operating_revenue, # 营业总收入\n",
    "    'EBIT':income.net_profit+income.financial_expense+income.income_tax_expense,\n",
    "    \n",
    "    'TOC':income.total_operating_cost,#营业总成本\n",
    "    'NOCF/MC':cash_flow.net_operate_cash_flow / valuation.market_cap, #经营活动产生的现金流量净额/总市值\n",
    "    'OTR':indicator.ocf_to_revenue, #经营活动产生的现金流量净额/营业收入(%) \n",
    "    \n",
    "    \n",
    "    'GPOA':indicator.gross_profit_margin * income.operating_revenue / balance.total_assets,  #毛利润 / 总资产 = 毛利率*营业收入 / 总资产\n",
    "    'GPM':indicator.gross_profit_margin, # 毛利率\n",
    "    'OPM':income.operating_profit / income.operating_revenue, #营业利润率\n",
    "    'NPM':indicator.net_profit_margin, # 净利率\n",
    "    'ROA':indicator.roa, # ROA\n",
    "    'ROE':indicator.roe, # ROE\n",
    "    'INC':indicator.inc_return, # 净资产收益率(扣除非经常损益)(%)\n",
    "    'EPS':indicator.eps, # 净资产收益率(扣除非经常损益)(%)\n",
    "    'AP':indicator.adjusted_profit, # 扣除非经常损益后的净利润(元)\n",
    "    'OP':indicator.operating_profit, # 经营活动净收益(元)\n",
    "    'VCP':indicator.value_change_profit, # 价值变动净收益(元) = 公允价值变动净收益+投资净收益+汇兑净收益\n",
    "    \n",
    "    'ETTR':indicator.expense_to_total_revenue, # 营业总成本/营业总收入(%)\n",
    "    'OPTTR':indicator.operation_profit_to_total_revenue, # 营业利润/营业总收入(%)\n",
    "    'NPTTR':indicator.net_profit_to_total_revenue, # 净利润/营业总收入(%)\n",
    "    'OETTR':indicator.operating_expense_to_total_revenue, # 营业费用/营业总收入\n",
    "    'GETTR':indicator.ga_expense_to_total_revenue, # 管理费用/营业总收入(%)\n",
    "    'FETTR':indicator.financing_expense_to_total_revenue, # 财务费用/营业总收入(%)\t\n",
    "    \n",
    "    'OPTP':indicator.operating_profit_to_profit, # 经营活动净收益/利润总额(%)\n",
    "    'IPTP':indicator.invesment_profit_to_profit, # 价值变动净收益/利润总额(%)\n",
    "    'GSASTR':indicator.goods_sale_and_service_to_revenue, # 销售商品提供劳务收到的现金/营业收入(%)\n",
    "    'OTR':indicator.ocf_to_revenue, # 经营活动产生的现金流量净额/营业收入(%)\n",
    "    'OTOP':indicator.ocf_to_operating_profit, # 经营活动产生的现金流量净额/经营活动净收益(%)\n",
    "    \n",
    "    'ITRYOY':indicator.inc_total_revenue_year_on_year, # 营业总收入同比增长率(%)\n",
    "    'ITRA':indicator.inc_total_revenue_annual, # 营业总收入环比增长率(%)\n",
    "    'IRYOY':indicator.inc_revenue_year_on_year, # 营业收入同比增长率(%)\n",
    "    'IRA':indicator.inc_revenue_annual, # 营业收入环比增长率(%)\n",
    "    'IOPYOY':indicator.inc_operation_profit_year_on_year, # 营业利润同比增长率(%)\n",
    "    'IOPA':indicator.inc_operation_profit_annual, # 营业利润环比增长率(%)\n",
    "    'INPYOY':indicator.inc_net_profit_year_on_year, # 净利润同比增长率(%)\n",
    "    'INPA':indicator.inc_net_profit_annual, # 净利润环比增长率(%)\n",
    "    'INPTSYOY':indicator.inc_net_profit_to_shareholders_year_on_year, # 归属母公司股东的净利润同比增长率(%)\n",
    "    'INPTSA':indicator.inc_net_profit_to_shareholders_annual, # 归属母公司股东的净利润环比增长率(%)\n",
    "    'INPTSA':indicator.inc_net_profit_to_shareholders_annual, # 归属母公司股东的净利润环比增长率(%)\n",
    "    \n",
    "    \n",
    "    'ROIC':(income.net_profit+income.financial_expense+income.income_tax_expense)/(balance.total_owner_equities+balance.shortterm_loan+balance.non_current_liability_in_one_year+balance.longterm_loan+balance.bonds_payable+balance.longterm_account_payable),\n",
    "    'OPTT':income.operating_profit / income.total_profit, # 营业利润占比\n",
    "    'TP/TOR':income.total_profit / income.total_operating_revenue, #利润总额/营业总收入\n",
    "    'OP/TOR':income.operating_profit / income.total_operating_revenue,\n",
    "    'NP/TOR':income.net_profit / income.total_operating_revenue,\n",
    "\n",
    "    'NP':income.net_profit, # 净利润\n",
    "    \n",
    "    'TA':balance.total_assets, # 总资产\n",
    "\n",
    "    'DER':balance.total_liability / balance.equities_parent_company_owners, # 产权比率 = 负债合计/归属母公司所有者权益合计\n",
    "    'FCFF/TNCL':(cash_flow.net_operate_cash_flow - cash_flow.net_invest_cash_flow) / balance.total_non_current_liability, #自由现金流比非流动负债\n",
    "    'NOCF/TL': cash_flow.net_operate_cash_flow / balance.total_liability, # 经营活动产生的现金流量净额/负债合计\n",
    "    'TCA/TCL':balance.total_current_assets / balance.total_current_liability, # 流动比率\n",
    "\n",
    "    'PE':valuation.pe_ratio, # PE 市盈率\n",
    "    'PB':valuation.pb_ratio, # PB 市净率\n",
    "    'PR':valuation.pcf_ratio, # PR 市现率\n",
    "    'PS':valuation.ps_ratio, # PS 市销率\n",
    "    \n",
    "    'TOR/TA':income.total_operating_revenue / balance.total_assets, #总资产周转率\n",
    "    'TOR/FA':income.total_operating_revenue / balance.fixed_assets, #固定资产周转率\n",
    "    'TOR/TCA':income.total_operating_revenue / balance.total_current_assets, #流动资产周转率\n",
    "    'LTL/OC':balance.longterm_loan / income.operating_cost, #长期借款/营业成本\n",
    "    \n",
    "    'TL/TA':balance.total_liability / balance.total_assets, #总资产/总负债\n",
    "    'TL/TOE':balance.total_liability / balance.total_owner_equities,#负债权益比\n",
    "    \n",
    "    }\n",
    "\n",
    "adjust_factors = {\n",
    "    'TOR/TA':income.total_operating_revenue / balance.total_assets, #总资产周转率\n",
    "    'TOR/FA':income.total_operating_revenue / balance.fixed_assets, #固定资产周转率\n",
    "    'TOR/TCA':income.total_operating_revenue / balance.total_current_assets, #流动资产周转率\n",
    "    'LTL/OC':balance.longterm_loan / income.operating_cost, #长期借款/营业成本\n",
    "    \n",
    "    'TL/TA':balance.total_liability / balance.total_assets, #总资产/总负债\n",
    "    'TL/TOE':balance.total_liability / balance.total_owner_equities,#负债权益比\n",
    "    \n",
    "    'DER':balance.total_liability / balance.equities_parent_company_owners, # 产权比率 = 负债合计/归属母公司所有者权益合计\n",
    "    'FCFF/TNCL':(cash_flow.net_operate_cash_flow - cash_flow.net_invest_cash_flow) / balance.total_non_current_liability, #自由现金流比非流动负债\n",
    "    'NOCF/TL': cash_flow.net_operate_cash_flow / balance.total_liability, # 经营活动产生的现金流量净额/负债合计\n",
    "    'TCA/TCL':balance.total_current_assets / balance.total_current_liability, # 流动比率\n",
    "    \n",
    "    'ROIC':(income.net_profit+income.financial_expense+income.income_tax_expense)/(balance.total_owner_equities+balance.shortterm_loan+balance.non_current_liability_in_one_year+balance.longterm_loan+balance.bonds_payable+balance.longterm_account_payable),\n",
    "    'OPTT':income.operating_profit / income.total_profit, # 营业利润占比\n",
    "    'TP/TOR':income.total_profit / income.total_operating_revenue, #利润总额/营业总收入\n",
    "    'OP/TOR':income.operating_profit / income.total_operating_revenue,\n",
    "    'NP/TOR':income.net_profit / income.total_operating_revenue,\n",
    "    \n",
    "    'NOCF/MC':cash_flow.net_operate_cash_flow / valuation.market_cap, #经营活动产生的现金流量净额/总市值\n",
    "    'GPOA':indicator.gross_profit_margin * income.operating_revenue / balance.total_assets,  #毛利润 / 总资产 = 毛利率*营业收入 / 总资产\n",
    "    'OPM':income.operating_profit / income.operating_revenue, #营业利润率\n",
    "    'EBIT':income.net_profit+income.financial_expense+income.income_tax_expense,\n",
    "\n",
    "}\n",
    "#获取所有因子列表\n",
    "factor_list = list(fac_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamental_data(securities,factor_list,ttm_factors, date):\n",
    "    '''\n",
    "    获取基本面数据,横截面数据，时间、股票、因子三个参数确定\n",
    "    获取的数据中含有Nan值，一般用行业均值填充\n",
    "    输入：\n",
    "    factor_list:list, 普通因子\n",
    "    ttm_factors:list, ttm因子，获取过去四个季度财报数据的和\n",
    "    date:str 或者 datetime.data, 获取数据的时间\n",
    "    securities：list,查询的股票\n",
    "    输出：\n",
    "    DataFrame,普通因子和ttm因子的合并，index为股票代码，values为因子值\n",
    "    '''\n",
    "    if len(factor_list) == 0:\n",
    "        return 'factors list is empty, please input data'\n",
    "    #获取查询的factor list\n",
    "    q = query(valuation.code)\n",
    "    for fac in factor_list:\n",
    "        q = q.add_column(fac_dict[fac])\n",
    "    q = q.filter(valuation.code.in_(securities))\n",
    "    fundamental_df = get_fundamentals(q,date)\n",
    "    fundamental_df.index = fundamental_df['code']\n",
    "    fundamental_df.columns = ['code'] + factor_list\n",
    "\n",
    "    if type(date) == str:\n",
    "        year = int(date[:4])\n",
    "        month_day = date[5:]\n",
    "    elif type(date) == datetime.date:\n",
    "        date = date.strftime('%Y-%m-%d')\n",
    "        year = int(date[:4])\n",
    "        month_day = date[5:]\n",
    "    else:\n",
    "        return 'input date error'\n",
    "    \n",
    "    if month_day < '05-01':\n",
    "        statdate_list = [str(year-2)+'q4', str(year-1)+'q1', str(year-1)+'q2', str(year-1)+'q3']\n",
    "    elif month_day >= '05-01' and month_day < '09-01':\n",
    "        statdate_list = [str(year-1)+'q1', str(year-1)+'q2', str(year-1)+'q3',str(year)+'q1']\n",
    "    elif month_day >= '09-01' and month_day < '11-01':\n",
    "        statdate_list = [str(year-1)+'q2', str(year-1)+'q3', str(year)+'q1', str(year)+'q2']\n",
    "    elif month_day >= '11-01':\n",
    "        statdate_list = [str(year-1)+'q4', str(year)+'q1', str(year)+'q2', str(year)+'q3']\n",
    "            \n",
    "    ttm_fundamental_data = ''\n",
    "   \n",
    "    ttm_q = query(valuation.code)\n",
    "    for fac in ttm_factors:\n",
    "        ttm_q = ttm_q.add_column(fac_dict[fac])\n",
    "    ttm_q = ttm_q.filter(valuation.code.in_(securities))  \n",
    "                             \n",
    "    for statdate in statdate_list:\n",
    "        if type(ttm_fundamental_data) == str:\n",
    "            fundamental_data = get_fundamentals(ttm_q, statDate=statdate)\n",
    "            fundamental_data.index = fundamental_data['code']\n",
    "            ttm_fundamental_data = fundamental_data\n",
    "        else:\n",
    "            fundamental_data = get_fundamentals(ttm_q, statDate=statdate)\n",
    "            fundamental_data.index = fundamental_data['code']\n",
    "            ttm_fundamental_data.iloc[:,1:] += fundamental_data.iloc[:,1:]\n",
    "    ttm_fundamental_data.columns = ['code'] + ttm_factors\n",
    "    results = pd.merge(fundamental_df,ttm_fundamental_data,on=['code'],how='inner')\n",
    "    results = results.sort_values(by='code')\n",
    "    results.index = results['code']\n",
    "    results = results.drop(['code'],axis=1)\n",
    "    #删除非数值列\n",
    "    columns = list(results.columns)\n",
    "    for column in columns:\n",
    "        if not(isinstance(results[column][0],int) or isinstance(results[column][0],float)):\n",
    "            results = results.drop([column],axis=1)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_fundamentals(securities, date):\n",
    "    '''\n",
    "    获取所有基本面因子\n",
    "    输入：\n",
    "    securies:list,查询的股票代码\n",
    "    date:str or datetime，查询的时间\n",
    "    输出：\n",
    "    fundamentals:dataframe,index为股票代码，values为因子值\n",
    "    '''\n",
    "    q = query(valuation,balance,cash_flow,income,indicator).filter(valuation.code.in_(securities))\n",
    "    fundamentals = get_fundamentals(q,date)\n",
    "    fundamentals.index = fundamentals['code']\n",
    "    #删除非数值列\n",
    "    columns = list(fundamentals.columns)\n",
    "    for column in columns:\n",
    "        if not(isinstance(fundamentals[column][0],int) or isinstance(fundamentals[column][0],float)):\n",
    "            fundamentals = fundamentals.drop([column],axis=1)\n",
    "    fundamentals = fundamentals.sort_index()\n",
    "    return fundamentals\n",
    "all_fundamentals = get_all_fundamentals(all_stocks,start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_industry(industry_name,date,output_csv = False):\n",
    "    '''\n",
    "    获取股票对应的行业\n",
    "    input：\n",
    "    industry_name: str, \n",
    "    \"sw_l1\": 申万一级行业\n",
    "    \"sw_l2\": 申万二级行业\n",
    "    \"sw_l3\": 申万三级行业\n",
    "    \"jq_l1\": 聚宽一级行业\n",
    "    \"jq_l2\": 聚宽二级行业\n",
    "    \"zjw\": 证监会行业\n",
    "    date:时间\n",
    "    output: DataFrame,index 为股票代码，columns 为所属行业代码\n",
    "    '''\n",
    "    industries = list(get_industries(industry_name).index)\n",
    "    all_securities = get_all_securities(date=date)   #获取当天所有股票代码\n",
    "    all_securities['industry_code'] = 1\n",
    "    for ind in industries:\n",
    "        industry_stocks = get_industry_stocks(ind,date)\n",
    "        #有的行业股票不在all_stocks列表之中\n",
    "        industry_stocks = set(all_securities) & set(industry_stocks)\n",
    "        all_securities['industry_code'][industry_stocks] = ind\n",
    "    stock_industry = all_securities['industry_code'].to_frame()\n",
    "    if output_csv == True:\n",
    "        stock_industry.to_csv('stock_industry.csv') #输出csv文件，股票对应行业\n",
    "    return stock_industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_industry(data,date,industry_name='sw_l1'):\n",
    "    '''\n",
    "    使用行业均值填充nan值\n",
    "    input:\n",
    "    data：DataFrame,输入数据，index为股票代码\n",
    "    date:string,时间必须和data数值对应时间一致\n",
    "    output：\n",
    "    DataFrame,缺失值用行业中值填充，无行业数据的用列均值填充\n",
    "    '''\n",
    "    stocks = list(data.index)\n",
    "    stocks_industry = get_stock_industry(industry_name,date)\n",
    "    stocks_industry_merge = data.merge(stocks_industry, left_index=True,right_index=True,how='left')\n",
    "    stocks_dropna = stocks_industry_merge.dropna()\n",
    "    columns = list(data.columns)\n",
    "    select_data = []\n",
    "    group_data = stocks_industry_merge.groupby('industry_code')\n",
    "    group_data_mean = group_data.mean()\n",
    "    group_data = stocks_industry_merge.merge(group_data_mean,left_on='industry_code',right_index=True,how='left')\n",
    "    for column in columns:\n",
    "\n",
    "        if type(data[column][0]) != str:\n",
    "\n",
    "            group_data[column+'_x'][pd.isnull(group_data[column+'_x'])] = group_data[column+'_y'][pd.isnull(group_data[column+'_x'])]\n",
    "            \n",
    "            group_data[column] = group_data[column+'_x']\n",
    "            #print(group_data.head())\n",
    "            select_data.append(group_data[column])\n",
    "            \n",
    "    result = pd.concat(select_data,axis=1)\n",
    "    #行业均值为Nan,用总体均值填充\n",
    "    mean = result.mean()\n",
    "    for i in result.columns:\n",
    "        result[i].fillna(mean[i],inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取日期列表\n",
    "def get_tradeday_list(start,end,frequency=None,count=None):\n",
    "    '''\n",
    "    input:\n",
    "    start:str or datetime,起始时间，与count二选一\n",
    "    end:str or datetime，终止时间\n",
    "    frequency:\n",
    "        str: day,month,quarter,halfyear,默认为day\n",
    "        int:间隔天数\n",
    "    count:int,与start二选一，默认使用start\n",
    "    '''\n",
    "    if isinstance(frequency,int):\n",
    "        all_trade_days = get_trade_days(start,end)\n",
    "        trade_days = all_trade_days[::frequency]\n",
    "        days = [datetime.datetime.strftime(i,'%Y-%m-%d') for i in trade_days]\n",
    "        return days\n",
    "    \n",
    "    if count != None:\n",
    "        df = get_price('000001.XSHG',end_date=end,count=count)\n",
    "    else:\n",
    "        df = get_price('000001.XSHG',start_date=start,end_date=end)\n",
    "    if frequency == None or frequency =='day':\n",
    "        days = df.index\n",
    "    else:\n",
    "        df['year-month'] = [str(i)[0:7] for i in df.index]\n",
    "        if frequency == 'month':\n",
    "            days = df.drop_duplicates('year-month').index\n",
    "        elif frequency == 'quarter':\n",
    "            df['month'] = [str(i)[5:7] for i in df.index]\n",
    "            df = df[(df['month']=='01') | (df['month']=='04') | (df['month']=='07') | (df['month']=='10') ]\n",
    "            days = df.drop_duplicates('year-month').index\n",
    "        elif frequency =='halfyear':\n",
    "            df['month'] = [str(i)[5:7] for i in df.index]\n",
    "            df = df[(df['month']=='01') | (df['month']=='06')]\n",
    "            days = df.drop_duplicates('year-month').index\n",
    "    trade_days = [datetime.datetime.strftime(i,'%Y-%m-%d') for i in days]\n",
    "    return trade_days\n",
    "tl = get_tradeday_list(start_date,end_date,frequency='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_list(begin_date, end_date):\n",
    "    '''\n",
    "    得到datetime类型时间序列\n",
    "    '''\n",
    "    dates = []\n",
    "    dt = datetime.datetime.strptime(begin_date,\"%Y-%m-%d\")\n",
    "    date = begin_date[:]\n",
    "    while date <= end_date:\n",
    "        dates.append(date)\n",
    "        dt += datetime.timedelta(days=1)\n",
    "        date = dt.strftime(\"%Y-%m-%d\")\n",
    "    return dates\n",
    "\n",
    "#去极值函数\n",
    "#mad中位数去极值法\n",
    "def filter_extreme_MAD(series,n): #MAD: 中位数去极值 \n",
    "    median = series.quantile(0.5)\n",
    "    new_median = ((series - median).abs()).quantile(0.50)\n",
    "    max_range = median + n*new_median\n",
    "    min_range = median - n*new_median\n",
    "    return np.clip(series,min_range,max_range)\n",
    "\n",
    "#进行标准化处理\n",
    "def winsorize(factor, std=3, have_negative = True):\n",
    "    '''\n",
    "    去极值函数 \n",
    "    factor:以股票code为index，因子值为value的Series\n",
    "    std为几倍的标准差，have_negative 为布尔值，是否包括负值\n",
    "    输出Series\n",
    "    '''\n",
    "    r=factor.dropna().copy()\n",
    "    if have_negative == False:\n",
    "        r = r[r>=0]\n",
    "    else:\n",
    "        pass\n",
    "    #取极值\n",
    "    edge_up = r.mean()+std*r.std()\n",
    "    edge_low = r.mean()-std*r.std()\n",
    "    r[r>edge_up] = edge_up\n",
    "    r[r<edge_low] = edge_low\n",
    "    return r\n",
    "\n",
    "#标准化函数：\n",
    "def standardize(s,ty=2):\n",
    "    '''\n",
    "    s为Series数据\n",
    "    ty为标准化类型:1 MinMax,2 Standard,3 maxabs \n",
    "    '''\n",
    "    data=s.dropna().copy()\n",
    "    if int(ty)==1:\n",
    "        re = (data - data.min())/(data.max() - data.min())\n",
    "    elif ty==2:\n",
    "        re = (data - data.mean())/data.std()\n",
    "    elif ty==3:\n",
    "        re = data/10**np.ceil(np.log10(data.abs().max()))\n",
    "    return re\n",
    "\n",
    "#数据去极值及标准化\n",
    "def winsorize_and_standarlize(data,qrange=[0.05,0.95],axis=0):\n",
    "    '''\n",
    "    input:\n",
    "    data:Dataframe or series,输入数据\n",
    "    qrange:list,list[0]下分位数，list[1]，上分位数，极值用分位数代替\n",
    "    '''\n",
    "    if isinstance(data,pd.DataFrame):\n",
    "        if axis == 0:\n",
    "            q_down = data.quantile(qrange[0])\n",
    "            q_up = data.quantile(qrange[1])\n",
    "            index = data.index\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q_up[n]] = q_up[n]\n",
    "                data[n][data[n] < q_down[n]] = q_down[n]\n",
    "            data = (data - data.mean())/data.std()\n",
    "            data = data.fillna(0)\n",
    "        else:\n",
    "            data = data.stack()\n",
    "            data = data.unstack(0)\n",
    "            q = data.quantile(qrange)\n",
    "            index = data.index\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q[n]] = q[n]\n",
    "            data = (data - data.mean())/data.std()\n",
    "            data = data.stack().unstack(0)\n",
    "            data = data.fillna(0)\n",
    "            \n",
    "    elif isinstance(data,pd.Series):\n",
    "        name = data.name\n",
    "        q = data.quantile(qrange)\n",
    "        data = np.clip(data,q.values[0],q.values[1])\n",
    "        data = (data - data.mean())/data.std()\n",
    "    return data\n",
    "    \n",
    "def neutralize(data,date,market_cap,industry_name='sw_l1'):\n",
    "    '''\n",
    "    中性化，使用行业和市值因子中性化\n",
    "    input：\n",
    "    data：DataFrame,index为股票代码，columns为因子，values为因子值\n",
    "    name:str,行业代码\n",
    "    \"sw_l1\": 申万一级行业\n",
    "    \"sw_l2\": 申万二级行业\n",
    "    \"sw_l3\": 申万三级行业\n",
    "    \"jq_l1\": 聚宽一级行业\n",
    "    \"jq_l2\": 聚宽二级行业\n",
    "    \"zjw\": 证监会行业\n",
    "    date:获取行业数据的时间\n",
    "    maket_cap:市值因子\n",
    "    '''\n",
    "    industry_se = get_stock_industry(industry_name,date)\n",
    "    columns = list(data.columns)\n",
    "    if isinstance(industry_se,pd.Series):\n",
    "        industry_se = industry_se.to_frame()\n",
    "    if isinstance(market_cap,pd.Series):\n",
    "        market_cap = market_cap.to_frame()\n",
    "        \n",
    "    index = list(data.index)\n",
    "    industry_se = np.array(industry_se.ix[index,0].tolist())\n",
    "    industry_dummy = sm.categorical(industry_se,drop=True)\n",
    "    industry_dummy = pd.DataFrame(industry_dummy,index=index)\n",
    "    market_cap = np.log(market_cap.loc[index])\n",
    "    x = pd.concat([industry_dummy,market_cap],axis=1)\n",
    "    model = sm.OLS(data,x)\n",
    "    result = model.fit()\n",
    "    y_fitted =  result.fittedvalues\n",
    "    neu_result = data - y_fitted\n",
    "    return neu_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算收益"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_profit(stocks,start_date,end_date,month_num=1,cal_num=3):\n",
    "    '''\n",
    "    获取月收益率数据，数据为本月相对于上月的增长率\n",
    "    input:\n",
    "    stocks:list 股票代码\n",
    "    start_date:str, 初始日期\n",
    "    end_date:str,终止日期\n",
    "    month_num:计算几个月的收益率，默认为1，即一个月的收益率\n",
    "    cal_num:int，计算每月最后n天的收盘价均值，默认为3\n",
    "    \n",
    "    '''\n",
    "    start_year = int(start_date[:4])\n",
    "    end_year = int(end_date[:4])\n",
    "    start_month = int(start_date[5:7])\n",
    "    end_month = int(end_date[5:7])\n",
    "    len_month = (end_year - start_year)*12 + (end_month - start_month)\n",
    "    price_list = []\n",
    "    #获取初始时间之前一个月的价格数据\n",
    "    if start_month == 1:\n",
    "        last_date = str(start_year-1)+'-'+'12'+'-'+'01'\n",
    "    else:\n",
    "        last_date = str(start_year-1)+'-'+str(start_month-1)+'-'+'01'\n",
    "    last_price = get_price(stocks,fields=['close'],count=cal_num,end_date=last_date)['close']\n",
    "    last_price = last_price.mean().to_frame()\n",
    "    last_price.columns = [last_date]\n",
    "    price_list.append(last_price)\n",
    "    #计算给定时间段内的月度价格数据\n",
    "    for i in range(len_month):\n",
    "        date = str(start_year+i//12)+'-'+str(start_month+i%12)+'-'+'01'\n",
    "        price = get_price(stocks,fields=['close'],count=cal_num,end_date=date)['close']\n",
    "        price_mean = price.mean().to_frame()\n",
    "        price_mean.columns = [date]\n",
    "        price_list.append(price_mean)\n",
    "    month_profit = pd.concat(price_list,axis=1)\n",
    "    #计算月度收益率\n",
    "    month_profit_pct = month_profit.pct_change(month_num,axis=1).dropna(axis=1,how='all')\n",
    "    return month_profit_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profit_depend_timelist(stocks,timelist,month_num=1,cal_num=3):\n",
    "    '''\n",
    "    input:\n",
    "    stocks:list 股票代码\n",
    "    timelist: 时间序列\n",
    "    month_num:计算几个月的收益率，默认为1，即一个月的收益率\n",
    "    cal_num:int，计算每月最后n天的收盘价均值，默认为3\n",
    "    '''\n",
    "    price_list = []\n",
    "    for date in timelist:\n",
    "        price = get_price(stocks,fields=['close'],count=cal_num,end_date=date)['close']\n",
    "        price_mean = price.mean().to_frame()\n",
    "        price_mean.columns = [date]\n",
    "        price_list.append(price_mean)\n",
    "    profit = pd.concat(price_list,axis=1)\n",
    "    profit_pct = profit.pct_change(month_num,axis=1).dropna(axis=1,how='all')\n",
    "    return profit_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_profit_forward(stocks,end_date,start_date=None,count=-1,pre_num=20):\n",
    "    '''\n",
    "    获取收益率,pre_num为计算时间差，在时间轴上的当期值是未来计算周期内的收益率，\n",
    "    例如：pre_num=3,2013-01-01对应的收益率是2013-01-04的收益率与01-01日收益率之差\n",
    "    input：\n",
    "    stocks:list or Series,股票代码\n",
    "    start_date:开始时间\n",
    "    end_date：结束时间\n",
    "    count:与start_date二选一，向前取值个数\n",
    "    pre_num:int,向后计算的天数\n",
    "    output:\n",
    "    profit:dataframe,index为日期，columns为股票代码，values为收益率\n",
    "    '''\n",
    "    if count == -1:\n",
    "        price = get_price(stocks,start_date,end_date,fields=['close'])['close']\n",
    "        date_list = get_trade_days(start_date=start_date,end_date=end_date)\n",
    "        price.index = date_list\n",
    "\n",
    "    else:\n",
    "        price = get_price(stocks,end_date=end_date,count=count,fields=['close'])['close']\n",
    "        date_list = get_trade_days(end_date=end_date,count=count)\n",
    "        price.index = date_list\n",
    "    profit = price.pct_change(periods=pre_num).shift(-pre_num).dropna()\n",
    "    return profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_day_data(stocks,factor_list,ttm_factors,date,neu=False):\n",
    "    '''\n",
    "    获取一天的基本面数据\n",
    "    input:\n",
    "    stocks:list,股票列表\n",
    "    factor_list:list,普通因子列表\n",
    "    ttm_factors:list,ttm因子列表\n",
    "    date:str or datetime， 获取数据时间\n",
    "    neu:bool,是否进行中性化处理，使用市值和行业进行中性化，默认不进行中性化\n",
    "    '''\n",
    "    fund_data = get_fundamental_data(stocks,factor_list,ttm_factors,date)\n",
    "    fillna_data = fillna_with_industry(fund_data,date)\n",
    "    if neu == False:\n",
    "        results = winsorize_and_standarlize(fillna_data)\n",
    "    elif 'MC' in fillna_data.columns:\n",
    "        neu_data = neutralize(fillna_data,date,fillna_data['MC'])\n",
    "        results = winsorize_and_standarlize(neu_data)\n",
    "    elif 'market_cap' in fillna_data.columns:\n",
    "        neu_data = neutralize(fillna_data,date,fillna_data['market_cap'])\n",
    "        results = winsorize_and_standarlize(neu_data)\n",
    "    else:\n",
    "        print(\"error: please input 'market_cap' for neutralize\")\n",
    "        return None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timelist_data(stocks,factor_list,ttm_factors,timelist,neu=False):\n",
    "    dic = {}\n",
    "    for date in timelist:\n",
    "        fund_date = get_one_day_data(stocks,factor_list,ttm_factors,date,neu=neu)\n",
    "        dic[date] = fund_date\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_data = get_timelist_data(all_stocks,factor_list,ttm_factors,tl)\n",
    "fund_data_neu = get_timelist_data(all_stocks,factor_list,ttm_factors,tl,neu=True)\n",
    "profit =  get_profit_depend_timelist(all_stocks,tl,month_num=2,cal_num=3)\n",
    "res = []\n",
    "res.append(fund_data)\n",
    "res.append(fund_data_neu)\n",
    "res.append(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据输出到pickle文件\n",
    "with open('fundamental_data.pkl','wb') as pk_file:\n",
    "    pickle.dump(res,pk_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二模块 因子选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from sklearn.feature_selection import RFE,SelectKBest,SelectPercentile,SelectFromModel,f_classif\n",
    "import lightgbm as lgb \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import gc\n",
    "from sklearn.metrics import accuracy_score,recall_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fundamental_data.pkl','rb') as pk_file:\n",
    "    data_pk = pickle.load(pk_file)\n",
    "fund_data = data_pk[0]\n",
    "fund_data_neu = data_pk[1]\n",
    "profit = data_pk[2]\n",
    "keys = list(fund_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#截面数据，将profit与基本面数据对齐，基本面数据对应下一月的profit\n",
    "def get_fund_profit_data(fund_data,profit):\n",
    "    '''\n",
    "    input:\n",
    "    fund_data:dic key为日期，values为dataframe，基本面数据，index为股票代码，columns为因子\n",
    "    profit:dataframe,index为股票代码，columns为时间\n",
    "    注意：此函数针对于fund_data keys日期与profit日期在位置上已经对应\n",
    "    '''\n",
    "    keys = list(fund_data.keys())\n",
    "    columns = profit.columns.tolist()\n",
    "    l = min(len(keys),len(columns))\n",
    "    fund_profit = {}\n",
    "    for i in range(l):\n",
    "        fd = fund_data[keys[i]].copy() #复制新的dataframe，否则fund_profit为引用，在fund_profit上修改值会直接影响到fund_data\n",
    "        p = profit[columns[i]].to_frame()\n",
    "        p.columns = ['profit']\n",
    "        fd_merge = pd.merge(fd,p,left_index=True,right_index=True,how='inner')\n",
    "        fund_profit[keys[i]] = fd_merge\n",
    "    return fund_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fund_profit_class_data(fund_data,profit):\n",
    "    '''\n",
    "    profit不再是数值，而是类别，大于0标记为1，小于0标记为0\n",
    "    input:\n",
    "    fund_data:dic key为日期，values为dataframe，基本面数据，index为股票代码，columns为因子\n",
    "    profit:dataframe,index为股票代码，columns为时间\n",
    "    output:\n",
    "    fund_profit:dic, 在fund_data每个dataframe后面加了profit列\n",
    "    注意：此函数针对于fund_data keys日期与profit日期在位置上已经对应\n",
    "    '''\n",
    "    pf = profit.copy(deep=True)\n",
    "    pf[pf>0] = 1\n",
    "    pf[pf<0] = 0\n",
    "    keys = list(fund_data.keys())\n",
    "    columns = pf.columns.tolist()\n",
    "    l = min(len(keys),len(columns))\n",
    "    fund_profit = {}\n",
    "    for i in range(l):\n",
    "        fd = fund_data[keys[i]].copy() #复制新的dataframe，否则fund_profit为引用，在fund_profit上修改值会直接影响到fund_data\n",
    "        p = pf[columns[i]].to_frame()\n",
    "        p.columns = ['profit']\n",
    "        fd_merge = pd.merge(fd,p,left_index=True,right_index=True,how='inner')\n",
    "        fund_profit[keys[i]] = fd_merge\n",
    "    return fund_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_profit_data = get_fund_profit_data(fund_data,profit)\n",
    "fund_profit_data_neu = get_fund_profit_data(fund_data_neu,profit)\n",
    "fund_profit_class_data = get_fund_profit_class_data(fund_data,profit)\n",
    "fund_profit_class_data_neu = get_fund_profit_class_data(fund_data_neu,profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因子评判函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用稳健回归（sm.RLM）robust linear model\n",
    "'''\n",
    "用回归系数作为因子有效性的指标，如果因子与收益之间非线性，则此指标不准确，此指标作为参考之一\n",
    "'''\n",
    "def get_RLM_res(fund_profit_data):\n",
    "    '''\n",
    "    input:\n",
    "    fund_profit_data:dic, keys为日期，values为dataframe，基本面数据，index为股票代码，columns为因子,columns最后一列为profit\n",
    "    output:\n",
    "    f: dataframe, index为因子，columns为时间，values为稳健回归系数\n",
    "    t: dataframe,index为因子，columns为时间，values为稳健回归系数的t值\n",
    "    '''\n",
    "    keys = fund_profit_data.keys()\n",
    "    \n",
    "    f_dic = {}\n",
    "    t_dic = {}\n",
    "    f = []\n",
    "    t = []\n",
    "    for k in keys:\n",
    "        col = fund_profit_data[k].columns\n",
    "        f_list = []\n",
    "        t_list = []\n",
    "        for c in col[:-1]:\n",
    "            df = fund_profit_data[k]\n",
    "            rlm_model = sm.RLM(df[col[-1]],df[c],M=sm.robust.norms.HuberT()).fit()\n",
    "            f_list.append(rlm_model.params)\n",
    "            t_list.append(rlm_model.tvalues)\n",
    "        f_list = [f_list[i].values[0] for i in range(len(f_list))]\n",
    "        t_list = [t_list[i].values[0] for i in range(len(t_list))]\n",
    "        f_df_k = pd.DataFrame(f_list,index=list(col[:-1]),columns=[k])\n",
    "        t_df_k = pd.DataFrame(t_list,index=col[:-1],columns=[k])\n",
    "        f.append(f_df_k)\n",
    "        t.append(t_df_k)\n",
    "    f = pd.concat(f,axis=1)\n",
    "    t = pd.concat(t,axis=1)\n",
    "    return f,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlm_res = get_RLM_res(fund_profit_data)\n",
    "rlm_neu_res = get_RLM_res(fund_profit_data_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "信息系数IC值，可以有效的观察到某个因子收益率预测的稳定性和动量特征，以便在组合优化时用作筛选的指标。常见的IC值计算方法有两种：\n",
    "相关系数（Pearson Correlation）和秩相关系数（Spearman Rank Correlation）,此例中IC值统计用到的是秩相关系数，\n",
    "与IC相关的用来判断因子的有效性和预测能力指标如下：\n",
    "IC值的均值\n",
    "IC值的标准差\n",
    "IC值大于0的比例\n",
    "IC绝对值大于0.02的比例\n",
    "IR （IC均值与IC标准差的比值）\n",
    "参考：https://www.joinquant.com/post/16105?tag=algorithm\n",
    "'''\n",
    "def get_IC(fund_profit_data):\n",
    "    '''\n",
    "    input:\n",
    "    fund_profit_data:dic, keys为日期，values为dataframe，基本面数据，index为股票代码，columns为因子,columns最后一列为profit\n",
    "    output:\n",
    "    p: dataframe, index为因子，columns为时间，values为pearson相关系数\n",
    "    s: dataframe,index为因子，columns为时间，values为spearman相关系数\n",
    "    '''\n",
    "    keys = fund_profit_data.keys()\n",
    "    p_dic = {}\n",
    "    s_dic = {}\n",
    "    p = []\n",
    "    s = []\n",
    "    for k in keys:\n",
    "        columns = fund_profit_data[k].columns\n",
    "        p_list = []\n",
    "        s_list = []\n",
    "        for c in columns[:-1]:\n",
    "            df = fund_profit_data[k]\n",
    "            p_value = df[c].corr(df[columns[-1]],method='pearson')\n",
    "            s_value = df[c].corr(df[columns[-1]],method='spearman')\n",
    "            p_list.append(p_value)\n",
    "            s_list.append(s_value)\n",
    "\n",
    "        p_df_k = pd.DataFrame(p_list,index=list(columns[:-1]),columns=[k])\n",
    "        s_df_k = pd.DataFrame(s_list,index=columns[:-1],columns=[k])\n",
    "        p.append(p_df_k)\n",
    "        s.append(s_df_k)\n",
    "    p = pd.concat(p,axis=1)\n",
    "    s = pd.concat(s,axis=1)\n",
    "    return p,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_res = get_IC(fund_profit_data)\n",
    "ic_res_neu = get_IC(fund_profit_data_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#好不容易跑完的数据，赶紧保存一下子\n",
    "res = []\n",
    "res.append(rlm_res)\n",
    "res.append(rlm_neu_res)\n",
    "res.append(ic_res)\n",
    "res.append(ic_res_neu)\n",
    "with open('judge_data.pkl','wb') as pf:\n",
    "    pickle.dump(res,pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算IC的各个指标\n",
    "def cal_IC_indicator(data):\n",
    "    '''\n",
    "    input:\n",
    "    data:dataframe,index为因子，columns为日期\n",
    "    output:\n",
    "    res:dataframe，index为因子，输出计算好的各个评判标准\n",
    "    '''\n",
    "    data = data.stack().unstack(0)\n",
    "    data_mean = data.mean()\n",
    "    data_std = data.std()\n",
    "    data_ir = data_mean / data_std\n",
    "    \n",
    "    data_mean_df = data_mean.to_frame()\n",
    "    data_mean_df.columns = ['mean']\n",
    "    data_std_df = data_std.to_frame()\n",
    "    data_std_df.columns = ['std']\n",
    "    data_ir_df = data_ir.to_frame()\n",
    "    data_ir_df.columns = ['IR']\n",
    "    \n",
    "    data_nagative = (data[data > 0]).count() / len(data)\n",
    "    data_nagative_df = data_nagative.to_frame()\n",
    "    data_nagative_df.columns = ['IC正值比例']\n",
    "    data_abs_dayu = (data[data.abs() > 0.02]).count() / len(data)\n",
    "    data_abs_dayu_df = data_abs_dayu.to_frame()\n",
    "    data_abs_dayu_df.columns = ['IC绝对值>0.02']\n",
    "    res = pd.concat([data_mean_df,data_std_df,data_nagative_df,data_abs_dayu_df,data_ir_df],axis=1)\n",
    "    return res    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_indicator_pearson = cal_IC_indicator(ic_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征选择方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection():\n",
    "    '''\n",
    "    特征选择：\n",
    "    identify_collinear：基于相关系数，删除小于correlation_threshold的特征\n",
    "    identify_importance_lgbm：基于LightGBM算法，得到feature_importance,选择和大于p_importance的特征\n",
    "    filter_select:单变量选择，指定k,selectKBest基于method提供的算法选择前k个特征，selectPercentile选择前p百分百的特征\n",
    "    wrapper_select:RFE，基于estimator递归特征消除，保留n_feature_to_select个特征\n",
    "    embedded_select： 基于estimator，\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.supports = None #bool型，特征是否被选中\n",
    "        self.columns = None  #选择的特征\n",
    "        self.record_collinear = None #自相关矩阵大于门限值\n",
    "        self.feature_importances = None #lgbm算法保存特征重要性值\n",
    "        \n",
    "    def identify_collinear(self, data, correlation_threshold):\n",
    "        \"\"\"\n",
    "        Finds collinear features based on the correlation coefficient between features. \n",
    "        For each pair of features with a correlation coefficient greather than `correlation_threshold`,\n",
    "        only one of the pair is identified for removal. \n",
    "\n",
    "        Using code adapted from: https://gist.github.com/Swarchal/e29a3a1113403710b6850590641f046c\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "\n",
    "        data : dataframe\n",
    "            Data observations in the rows and features in the columns\n",
    "\n",
    "        correlation_threshold : float between 0 and 1\n",
    "            Value of the Pearson correlation cofficient for identifying correlation features\n",
    "\n",
    "        \"\"\"\n",
    "        columns = data.columns\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "\n",
    "        # Calculate the correlations between every column\n",
    "        corr_matrix = data.corr()\n",
    "        \n",
    "        self.corr_matrix = corr_matrix\n",
    "    \n",
    "        # Extract the upper triangle of the correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "        # Select the features with correlations above the threshold\n",
    "        # Need to use the absolute value\n",
    "        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "        obtain_columns = [column for column in columns if column not in to_drop]\n",
    "        self.columns = obtain_columns\n",
    "        # Dataframe to hold correlated pairs\n",
    "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        # Iterate through the columns to drop\n",
    "        for column in to_drop:\n",
    "\n",
    "            # Find the correlated features\n",
    "            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "            # Find the correlated values\n",
    "            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "            # Record the information (need a temp df for now)\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                             'corr_feature': corr_features,\n",
    "                                             'corr_value': corr_values})\n",
    "\n",
    "            # Add to dataframe\n",
    "            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "\n",
    "        self.record_collinear = record_collinear\n",
    "        return data[obtain_columns]\n",
    "     \n",
    "        \n",
    "    def identify_importance_lgbm(self, features, labels,p_importance=0.8, eval_metric='auc', task='classification', \n",
    "                                 n_iterations=10, early_stopping = True):\n",
    "       \n",
    "\n",
    "        # One hot encoding\n",
    "        data = features\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        # Extract feature names\n",
    "        feature_names = list(features.columns)\n",
    "\n",
    "        # Convert to np array\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels).reshape((-1, ))\n",
    "\n",
    "        # Empty array for feature importances\n",
    "        feature_importance_values = np.zeros(len(feature_names))\n",
    "        \n",
    "        print('Training Gradient Boosting Model\\n')\n",
    "        \n",
    "        # Iterate through each fold\n",
    "        for _ in range(n_iterations):\n",
    "\n",
    "            if task == 'classification':\n",
    "                model = lgb.LGBMClassifier(n_estimators=100, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            elif task == 'regression':\n",
    "                model = lgb.LGBMRegressor(n_estimators=100, learning_rate = 0.05, verbose = -1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Task must be either \"classification\" or \"regression\"')\n",
    "                \n",
    "            # If training using early stopping need a validation set\n",
    "            if early_stopping:\n",
    "                \n",
    "                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15)\n",
    "\n",
    "                # Train the model with early stopping\n",
    "                model.fit(train_features, train_labels, eval_metric = eval_metric,\n",
    "                          eval_set = [(valid_features, valid_labels)],\n",
    "                           verbose = -1)\n",
    "                \n",
    "                # Clean up memory\n",
    "                gc.enable()\n",
    "                del train_features, train_labels, valid_features, valid_labels\n",
    "                gc.collect()\n",
    "                \n",
    "            else:\n",
    "                model.fit(features, labels)\n",
    "\n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_ / n_iterations\n",
    "\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "        \n",
    "        # Sort features according to importance\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "        \n",
    "        # Normalize the feature importances to add up to one\n",
    "        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "        #obtain feature importance\n",
    "        self.feature_importances = feature_importances\n",
    "        select_df = feature_importances[feature_importances['cumulative_importance']<=p_importance]\n",
    "        select_columns = select_df['feature']\n",
    "        self.columns = list(select_columns.values)\n",
    "        res = data[self.columns]\n",
    "        return res\n",
    "        \n",
    "    def filter_select(self, data_x, data_y, k=None, p=50,method=f_classif):\n",
    "        columns = data_x.columns\n",
    "        if k != None:\n",
    "            model = SelectKBest(method,k)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        else:\n",
    "            model = SelectPercentile(method,p)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        self.support_ = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def wrapper_select(self,data_x,data_y,n,estimator):\n",
    "        columns = data_x.columns\n",
    "        model = RFE(estimator=estimator,n_features_to_select=n)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support() #标识被选择的特征在原数据中的位置\n",
    "        self.supports = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def embedded_select(self,data_x,data_y,estimator,threshold=None):\n",
    "        columns = data_x.columns\n",
    "        model = SelectFromModel(estimator=estimator,prefit=False,threshold=threshold)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support()\n",
    "        self.supports = supports\n",
    "        self.columns = columns[supports]\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用特征选择方法选择因子值\n",
    "test_fund = fund_profit_class_data_neu[keys[0]] #取一期数据测试\n",
    "test_fund1 = fund_profit_class_data_neu[keys[1]]\n",
    "#test_fund = pd.concat([test_fund0,test_fund1])\n",
    "columns = test_fund.columns\n",
    "fs = FeatureSelection()\n",
    "x = test_fund[columns[:-1]]\n",
    "y = test_fund[columns[-1]]\n",
    "\n",
    "lgbm = fs.identify_importance_lgbm(x,y) #使用特征选择方法选择有效因子\n",
    "fs.feature_importances #各个因子重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用机器学习算法探索选股或择时策略（示例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "以下代码是示例代码，简单走一遍机器学习探索策略及调参，具体有效的策略请大家自己探索，不做分享\n",
    "'''\n",
    "\n",
    "fund_data_train = fund_profit_class_data_neu[keys[1]]\n",
    "columns_s  = lgbm.columns\n",
    "col_s = columns_s[:-1]\n",
    "fund_data_train_y = fund_data_train[fund_data_train.columns[-1]]\n",
    "lgbm_x_train,lgbm_x_test,lgbm_y_train,lgbm_y_test = train_test_split(fund_data_train[col_s],fund_data_train_y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgbm_svm = SVC(max_iter=1000)\n",
    "param_grid = {'C':[0.1,1,3],'kernel':['rbf','sigmoid','linear','poly'],'gamma':np.arange(0.3,0.8,0.1)}\n",
    "lgbm_model = GridSearchCV(estimator=lgbm_svm,param_grid=param_grid,scoring='accuracy')\n",
    "lgbm_model.fit(lgbm_x_train,lgbm_y_train)\n",
    "lgbm_test_res = lgbm_model.predict(lgbm_x_test)\n",
    "accuracy = accuracy_score(lgbm_y_test,lgbm_test_res)\n",
    "print('accuracy is: %0.5f'%accuracy)\n",
    "print(lgbm_model.best_params_)\n",
    "print(lgbm_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier()\n",
    "gbdt_params_grid = {'max_depth':[4,6,8],'min_samples_split':[10,20,30]}\n",
    "gbdt_model = GridSearchCV(estimator=gbdt,param_grid=gbdt_params_grid)\n",
    "gbdt_model.fit(lgbm_x_train,lgbm_y_train)\n",
    "gbdt_test_res = gbdt_model.predict(lgbm_x_test)\n",
    "gbdt_accuracy = accuracy_score(lgbm_y_test,gbdt_test_res)\n",
    "print('accuracy is: %0.5f'%gbdt_accuracy)\n",
    "print(gbdt_model.best_params_)\n",
    "print(gbdt_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_for_pre = fund_profit_class_data_neu[keys[3]] #取一期的截面数据验证\n",
    "columns_for_pre = fund_for_pre.columns\n",
    "x_for_pre = fund_for_pre[col_s]\n",
    "y_for_pre = fund_for_pre[columns_for_pre[-1]]\n",
    "prediction = lgbm_model.predict(x_for_pre)\n",
    "accuracy_for_pre = accuracy_score(y_for_pre,prediction)\n",
    "print(accuracy_for_pre)\n",
    "print(len(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "MarkDown菜单",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
